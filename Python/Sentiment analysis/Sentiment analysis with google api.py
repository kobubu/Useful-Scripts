import logging
import requests
import json
import pandas as pd
import datetime
import os
from tqdm import tqdm
from google_auth_oauthlib.flow import InstalledAppFlow
from concurrent.futures import ThreadPoolExecutor, as_completed
import time


#–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
SCOPES = ['https://www.googleapis.com/auth/cloud-language'] #—Ç—É—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –º–µ–Ω—è—Ç—å
#—É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –∫–ª—é—á—É OAuth 2.0
CLIENT_SECRETS_FILE = r"PATH"
OUTPUT_DIR = r"C:/Users/Igor/Downloads/Telegram Desktop/"  # –ü–∞–ø–∫–∞ c —Ñ–∞–π–ª–æ–º
#–£–∫–∑–∞—Ç—å –∏–º—è —Ñ–∞–π–ª–∞
EXCEL_FILE = OUTPUT_DIR + r"FILENAME.xlsx"
#–£–∫–∞–∑–∞—Ç—å —è–∑—ã–∫ –ª—é–±–æ–π –∏–∑ available_languages
language = "de"

available_lanuages = '''
V2 Model
Language 	ISO-639-1 Code
Chinese (Simplified) 	zh
Chinese (Traditional) 	zh-Hant
Dutch 	nl
English 	en
French 	fr
German 	de
Italian 	it
Japanese 	ja
Korean 	ko
Portuguese (Brazilian & Continental) 	pt
Russian 	ru
Spanish 	es
'''


# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
LOG_FILENAME = OUTPUT_DIR+'processing_log.txt'
logging.basicConfig(filename=LOG_FILENAME, level=logging.INFO,
                    format='%(asctime)s - %(message)s',
                    filemode='w', encoding='utf-8')

timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
base_filename = os.path.splitext(os.path.basename(EXCEL_FILE))[0]  # –ë–µ—Ä–µ–º –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
filtered_filename = f"{base_filename}_filtered_{timestamp}.xlsx"
full_filename = f"{base_filename}_full_{timestamp}.xlsx"
filtered_filepath = os.path.join(OUTPUT_DIR, filtered_filename)
full_filepath = os.path.join(OUTPUT_DIR, full_filename)
request_count = 0

# –ü–æ–ª—É—á–µ–Ω–∏–µ OAuth 2.0 credentials
def get_oauth2_credentials():
    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, scopes=SCOPES)
    credentials = flow.run_local_server(port=0)
    return credentials

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
credentials = get_oauth2_credentials()
access_token = credentials.token

#—Ñ-—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å –∏–Ω—Ñ—ã
def log_and_print(message):
    logging.info(message)
    print(message)

# –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
def analyze_text(text, language):  # –Ø–∑—ã–∫ –∑–∞–¥–∞–µ—Ç—Å—è —è–≤–Ω–æ
    global request_count
    url = "https://language.googleapis.com/v2/documents:annotateText"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json; charset=utf-8"
    }
    features = {
        "extractEntities": True,
        "extractDocumentSentiment": True,
        "classifyText": True,
        "extractEntitySentiment": True,
        "moderateText": True
    }
    document = {
        "type": "PLAIN_TEXT",
        "content": text,
        "languageCode": language
    }
    body = {
        "document": document,
        "features": features,
        "encodingType": "UTF8"
    }

    request_count += 1
    log_and_print(f"–ó–∞–ø—Ä–æ—Å ‚Ññ {request_count} –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text}")

    response = requests.post(url, headers=headers, data=json.dumps(body))
    if response.status_code == 200:
        return response.json()
    else:
        return {"error": f"API error: {response.text}"}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
def process_row(index, row):
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è NaN
    word = str(row['#Word']) if not pd.isna(row['#Word']) else ""
    tips = str(row['#Tips']) if not pd.isna(row['#Tips']) else ""

    words = [(word, "Word"), (tips, "Tips")]
    results = []

    for word, source in words:
        if word.strip() == "":
            continue

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (Sentiment, Classification, Entities, Moderation)
        result = analyze_text(word, language)
        if "error" in result:
            log_and_print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ '{word}': {result['error']}")
            continue

        logging.info(f"–û—Ç–≤–µ—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{word}': –°—Ç–∞—Ç—É—Å - {result.get('status', 'N/A')}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º Sentiment
        sentiment_score = result.get("documentSentiment", {}).get("score", 0)
        sentiment_magnitude = result.get("documentSentiment", {}).get("magnitude", 0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        entities = result.get("entities", [])
        is_proper_noun = any(entity.get("type") == "PROPER" for entity in entities)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (Classification)
        categories = [f"{category['name']} ({round(category['confidence'], 3)})"
                      for category in result.get("categories", [])]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ (Moderation)
        moderation = [f"{mod['name']} ({round(mod['confidence'], 3)})"
                      for mod in result.get("moderationCategories", [])
                      if mod["name"] in dangerous_categories and mod["confidence"] >= 0.1]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
        row_data = {
            "Index": index,  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
            "#Level": row["#Level"],
            "Source": source,  # –£–∫–∞–∑—ã–≤–∞–µ—Ç, –æ—Ç–∫—É–¥–∞ —Å–ª–æ–≤–æ (Word –∏–ª–∏ Tips)
            "Text": word,
            "Entities": "\n".join(f"Entity: {entity['name']}, Type: {entity['type']}" for entity in entities),
            "Is Proper Noun": "Yes" if is_proper_noun else "No",
            "Sentiment Score": sentiment_score,
            "Sentiment Magnitude": sentiment_magnitude,
            "Categories": ", ".join(categories),
            "Moderation": ", ".join(moderation)
        }
        results.append(row_data)

    return results

# –ó–∞–≥—Ä—É–∂–∞–µ–º Excel-—Ñ–∞–π–ª
df = pd.read_excel(EXCEL_FILE)

if "#Word" not in df.columns and "#Tips" not in df.columns and "#Level" not in df.columns:
    raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ '#Level', '#Word' –∏ '#Tips'")

df["Index"] = range(len(df))

dangerous_categories = [
    "Toxic", "Insult", "Profanity", "Derogatory", "Sexual", "Death, Harm & Tragedy",
    "Violent", "Firearms & Weapons", "Religion & Belief", "Illicit Drugs", "War & Conflict", "Politics"
]

filtered_results = []
full_results = []

log_and_print(f'–í—ã–±—Ä–∞–Ω —è–∑—ã–∫ {language}')
log_and_print(f'–í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª {EXCEL_FILE}')
requests_count = 0

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(process_row, index, row) for index, row in df.iterrows()]

    for future in tqdm(as_completed(futures), total=len(futures), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫"):
        results = future.result()
        for row_data in results:
            full_results.append(row_data)
            if row_data["Sentiment Score"] <= -0.2 or row_data["Moderation"] or row_data["Is Proper Noun"] == "Yes":
                filtered_results.append(row_data)

        requests_count += 1
        time.sleep(0.1)  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –∫–≤–æ—Ç—ã

log_and_print(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {requests_count}")

df_filtered = pd.DataFrame(filtered_results).sort_values(by="Index")
df_full = pd.DataFrame(full_results).sort_values(by="Index")

df_filtered.drop(columns=["Index"], inplace=True)
df_full.drop(columns=["Index"], inplace=True)


df_filtered.to_excel(filtered_filepath, index=False)
df_full.to_excel(full_filepath, index=False)

log_and_print(
    f"\n‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df_filtered)} —Å—Ç—Ä–æ–∫. –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\nüîπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π: {filtered_filepath}\nüîπ –ü–æ–ª–Ω—ã–π: {full_filepath}\n"
)
