import requests
import json
import pandas as pd
import datetime
import os
from tqdm import tqdm
from google_auth_oauthlib.flow import InstalledAppFlow
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OAuth 2.0
SCOPES = ['https://www.googleapis.com/auth/cloud-language']
CLIENT_SECRETS_FILE = r"C:/–¥–∞–º–ø –±–∞–∑—ã/client_secret_988095010670-ga9ucce0koo8ddac4g7fc0huggj5fvre.apps.googleusercontent.com.json"  # –£–∫–∞–∂–∏ —Å–≤–æ–π –ø—É—Ç—å
EXCEL_FILE = r"C:/Users/Igor/Downloads/Telegram Desktop/IN_1926_UI_Crossword_Master_–ü—Ä–æ–≤–µ—Ä–∫–∞_—É—Ä–æ–≤–Ω–µ–π_161_200_FR_part3_800-1177.xlsx"  # –£–∫–∞–∂–∏ —Å–≤–æ–π –ø—É—Ç—å
OUTPUT_DIR = r"C:/Users/Igor/Downloads/Telegram Desktop/"  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º
timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
base_filename = os.path.splitext(os.path.basename(EXCEL_FILE))[0]  # –ë–µ—Ä–µ–º –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
filtered_filename = f"{base_filename}_filtered_{timestamp}.xlsx"
full_filename = f"{base_filename}_full_{timestamp}.xlsx"
filtered_filepath = os.path.join(OUTPUT_DIR, filtered_filename)
full_filepath = os.path.join(OUTPUT_DIR, full_filename)

# –ü–æ–ª—É—á–µ–Ω–∏–µ OAuth 2.0 credentials
def get_oauth2_credentials():
    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, scopes=SCOPES)
    credentials = flow.run_local_server(port=0)
    return credentials

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
credentials = get_oauth2_credentials()
access_token = credentials.token

# –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ (Sentiment, Classification, Moderation, Entities)
def analyze_text(text):
    url = "https://language.googleapis.com/v2/documents:annotateText"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json; charset=utf-8"
    }
    features = {
        "extractEntities": True,  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        "extractDocumentSentiment": True,  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç
        "classifyText": True,  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        "extractEntitySentiment": True,  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π
        "moderateText": True  # –ú–æ–¥–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    }
    document = {
        "type": "PLAIN_TEXT",
        "content": text,
        "languageCode": "fr"  # –ó–∞–¥–∞–µ–º —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π —è–∑—ã–∫ —è–≤–Ω–æ
    }
    body = {
        "document": document,
        "features": features,
        "encodingType": "UTF8"
    }
    response = requests.post(url, headers=headers, data=json.dumps(body))
    if response.status_code == 200:
        return response.json()
    else:
        return {"error": f"API error: {response.text}"}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
def process_row(row):
    words = [(row['#Word'], "Word"), (row['#Tips'], "Tips")]
    results = []

    for word, source in words:
        if pd.isna(word) or word.strip() == "":
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (Sentiment, Classification, Entities, Moderation)
        result = analyze_text(word)
        if "error" in result:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ '{word}': {result['error']}")
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º Sentiment
        sentiment_score = result.get("documentSentiment", {}).get("score", 0)
        sentiment_magnitude = result.get("documentSentiment", {}).get("magnitude", 0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        entities = result.get("entities", [])
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –∏–º–µ–Ω–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º
        is_proper_noun = any(entity.get("type") == "PROPER" for entity in entities)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (Classification)
        categories = [f"{category['name']} ({round(category['confidence'], 3)})"
                      for category in result.get("categories", [])]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ (Moderation)
        moderation = [f"{mod['name']} ({round(mod['confidence'], 3)})"
                      for mod in result.get("moderationCategories", [])
                      if mod["name"] in dangerous_categories and mod["confidence"] >= 0.1]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
        row_data = {
            "#Level": row["#Level"],
            "Source": source,  # –£–∫–∞–∑—ã–≤–∞–µ—Ç, –æ—Ç–∫—É–¥–∞ —Å–ª–æ–≤–æ (Word –∏–ª–∏ Tips)
            "Text": word,
            "Entities": ", ".join(entity["name"] for entity in entities),
            "Is Proper Noun": "Yes" if is_proper_noun else "No",
            "Sentiment Score": sentiment_score,
            "Sentiment Magnitude": sentiment_magnitude,
            "Categories": ", ".join(categories),
            "Moderation": ", ".join(moderation)
        }
        results.append(row_data)

    return results

# –ó–∞–≥—Ä—É–∂–∞–µ–º Excel-—Ñ–∞–π–ª
df = pd.read_excel(EXCEL_FILE)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
if "#Word" not in df.columns or "#Tips" not in df.columns or "#Level" not in df.columns:
    raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ '#Level', '#Word' –∏ '#Tips'")

# –û–ø–∞—Å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º
dangerous_categories = [
    "Toxic", "Insult", "Profanity", "Derogatory", "Sexual", "Death, Harm & Tragedy",
    "Violent", "Firearms & Weapons", "Religion & Belief", "Illicit Drugs", "War & Conflict", "Politics"
]

# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
filtered_results = []
full_results = []

# –†–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—Ç—Ä–æ–∫
with ThreadPoolExecutor(max_workers=10) as executor:  # –£–≤–µ–ª–∏—á—å max_workers, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤
    futures = [executor.submit(process_row, row) for _, row in df.iterrows()]

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    for future in tqdm(as_completed(futures), total=len(futures), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫"):
        results = future.result()
        for row_data in results:
            full_results.append(row_data)
            # –£—Å–ª–æ–≤–∏—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:
            # 1. –ï—Å–ª–∏ Sentiment Score <= -0.2
            # 2. –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ—Ä–∞—Ü–∏–µ–π
            # 3. –ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç—å —è–≤–ª—è–µ—Ç—Å—è –∏–º–µ–Ω–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º
            if row_data["Sentiment Score"] <= -0.2 or row_data["Moderation"] or row_data["Is Proper Noun"] == "Yes":
                filtered_results.append(row_data)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
df_filtered = pd.DataFrame(filtered_results)
df_full = pd.DataFrame(full_results)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
df_filtered.to_excel(filtered_filepath, index=False)
df_full.to_excel(full_filepath, index=False)

print(
    f"\n‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df_filtered)} —Å—Ç—Ä–æ–∫. –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\nüîπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π: {filtered_filepath}\nüîπ –ü–æ–ª–Ω—ã–π: {full_filepath}"
)
